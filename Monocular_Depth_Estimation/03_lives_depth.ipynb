{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch-gpu 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3070\n",
      "CUDA is available. Device name: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # True면 CUDA GPU가 인식됨\n",
    "print(torch.cuda.current_device())  # 현재 사용 중인 CUDA 디바이스\n",
    "print(torch.cuda.get_device_name(0)) \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Device name:\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### depth map live 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to Intel/dpt-large and revision e93beec (https://huggingface.co/Intel/dpt-large).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 깊이 추정 파이프라인 초기화\n",
    "# \"cuda\"가 사용 가능하면 GPU를 사용, 그렇지 않으면 CPU를 사용\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\"depth-estimation\", device=device)\n",
    "\n",
    "# 웹캠을 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # OpenCV 프레임을 PIL 이미지로 변환\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "\n",
    "    # 깊이 맵 추정\n",
    "    depth_map = pipe(pil_image)['depth']\n",
    "    depth_map = np.array(depth_map)\n",
    "\n",
    "    # 포인트 클라우드 생성 (예시)\n",
    "    # 여기에 포인트 클라우드 생성 코드를 추가하세요\n",
    "\n",
    "    # 결과를 화면에 표시\n",
    "    cv2.imshow('Depth Map', depth_map)\n",
    "    \n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원본, depth map(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to Intel/dpt-large and revision e93beec (https://huggingface.co/Intel/dpt-large).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import open3d as o3d\n",
    "\n",
    "# 깊이 추정 파이프라인 초기화\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\"depth-estimation\", device=device)\n",
    "\n",
    "# 웹캠을 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "def create_rgbd_image(rgb_image, depth_map):\n",
    "    # 깊이 맵 크기를 맞추기 위해 RGB 이미지 크기 조정\n",
    "    rgb_image = cv2.resize(rgb_image, (depth_map.shape[1], depth_map.shape[0]))\n",
    "    \n",
    "    # RGB와 깊이 이미지를 numpy 배열로 변환\n",
    "    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        o3d.geometry.Image(rgb_image),\n",
    "        o3d.geometry.Image(depth_map),\n",
    "        convert_rgb_to_intensity=False\n",
    "    )\n",
    "    \n",
    "    return rgbd_image\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # OpenCV 프레임을 PIL 이미지로 변환\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "\n",
    "    # 깊이 맵 추정\n",
    "    depth_map = pipe(pil_image)['depth']\n",
    "    depth_map = np.array(depth_map)\n",
    "\n",
    "    # RGBD 이미지 생성\n",
    "    rgbd_image = create_rgbd_image(rgb_frame, depth_map)\n",
    "    \n",
    "    # RGB 이미지와 깊이 맵을 OpenCV를 사용해 나란히 표시\n",
    "    depth_colormap = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "    depth_colormap = cv2.applyColorMap(depth_colormap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    combined_image = np.hstack((cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR), depth_colormap))\n",
    "    \n",
    "    cv2.imshow('RGB and Depth Map', combined_image)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to Intel/dpt-large and revision e93beec (https://huggingface.co/Intel/dpt-large).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 깊이 추정 파이프라인 초기화\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\"depth-estimation\", device=device)\n",
    "\n",
    "# 웹캠을 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "def create_rgbd_image(rgb_image, depth_map):\n",
    "    # 깊이 맵 크기를 맞추기 위해 RGB 이미지 크기 조정\n",
    "    rgb_image = cv2.resize(rgb_image, (depth_map.shape[1], depth_map.shape[0]))\n",
    "    \n",
    "    # 깊이 맵을 3채널로 변환\n",
    "    depth_colormap = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "    depth_colormap = cv2.applyColorMap(depth_colormap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    return depth_colormap\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # OpenCV 프레임을 PIL 이미지로 변환\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "\n",
    "    # 깊이 맵 추정\n",
    "    depth_map = pipe(pil_image)['depth']\n",
    "    depth_map = np.array(depth_map)\n",
    "    \n",
    "    # 흑백 깊이 맵 생성\n",
    "    depth_map_gray = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "\n",
    "    # RGB 깊이 맵 생성\n",
    "    depth_map_rgb = create_rgbd_image(rgb_frame, depth_map)\n",
    "    \n",
    "    # 원본, 흑백 깊이 맵, RGB 깊이 맵을 하나의 창에 나란히 배치\n",
    "    rgb_frame_bgr = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR)\n",
    "    combined_image = np.hstack((rgb_frame_bgr, cv2.cvtColor(depth_map_gray, cv2.COLOR_GRAY2BGR), depth_map_rgb))\n",
    "    \n",
    "    cv2.imshow('Camera | Grayscale Depth Map | RGB Depth Map', combined_image)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 포인트 클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to Intel/dpt-large and revision e93beec (https://huggingface.co/Intel/dpt-large).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import open3d as o3d\n",
    "\n",
    "# 깊이 추정 파이프라인 초기화\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\"depth-estimation\", device=device)\n",
    "\n",
    "# 웹캠을 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "def create_point_cloud(rgb_image, depth_map):\n",
    "    # 깊이 맵 크기를 맞추기 위해 RGB 이미지 크기 조정\n",
    "    rgb_image = cv2.resize(rgb_image, (depth_map.shape[1], depth_map.shape[0]))\n",
    "    \n",
    "    # RGB와 깊이 이미지를 numpy 배열로 변환\n",
    "    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        o3d.geometry.Image(rgb_image),\n",
    "        o3d.geometry.Image(depth_map),\n",
    "        convert_rgb_to_intensity=False\n",
    "    )\n",
    "    \n",
    "    # 카메라 매트릭스를 정의합니다 (여기서는 예시로 단순한 매트릭스를 사용)\n",
    "    intrinsic = o3d.camera.PinholeCameraIntrinsic(\n",
    "        o3d.camera.PinholeCameraIntrinsicParameters.PrimeSenseDefault\n",
    "    )\n",
    "    \n",
    "    # 포인트 클라우드를 생성합니다\n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "        rgbd_image,\n",
    "        intrinsic\n",
    "    )\n",
    "    \n",
    "    # 포인트 클라우드를 원점에 맞추기 위해 좌표계를 변환합니다\n",
    "    pcd.transform([[1, 0, 0, 0],\n",
    "                   [0, -1, 0, 0],\n",
    "                   [0, 0, -1, 0],\n",
    "                   [0, 0, 0, 1]])\n",
    "    \n",
    "    return pcd\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # OpenCV 프레임을 PIL 이미지로 변환\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "\n",
    "    # 깊이 맵 추정\n",
    "    depth_map = pipe(pil_image)['depth']\n",
    "    depth_map = np.array(depth_map)\n",
    "\n",
    "    # 포인트 클라우드 생성\n",
    "    pcd = create_point_cloud(rgb_frame, depth_map)\n",
    "    \n",
    "    # 포인트 클라우드 시각화\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    # 결과를 화면에 표시\n",
    "    cv2.imshow('RGB Frame', frame)\n",
    "    \n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 깊이에 따른 포인트 클라우드 색상 (Red-Violet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import open3d as o3d\n",
    "\n",
    "# 깊이 추정 파이프라인 초기화\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\"depth-estimation\", device=device)\n",
    "\n",
    "# 웹캠을 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "def create_point_cloud(rgb_image, depth_map):\n",
    "    # 깊이 맵 크기를 맞추기 위해 RGB 이미지 크기 조정\n",
    "    rgb_image = cv2.resize(rgb_image, (depth_map.shape[1], depth_map.shape[0]))\n",
    "    \n",
    "    # 깊이 값을 정규화\n",
    "    depth_min, depth_max = depth_map.min(), depth_map.max()\n",
    "    depth_map_normalized = (depth_map - depth_min) / (depth_max - depth_min)\n",
    "    \n",
    "    # 깊이 값에 따라 색상을 매핑하는 함수\n",
    "    def depth_to_color(value):\n",
    "        colors = [\n",
    "            (148, 0, 211),  # Violet\n",
    "            (75, 0, 130),   # Indigo\n",
    "            (0, 0, 255),    # Blue\n",
    "            (0, 255, 0),    # Green\n",
    "            (255, 255, 0),  # Yellow\n",
    "            (255, 127, 0),  # Orange\n",
    "            (255, 0, 0)     # Red\n",
    "        ]\n",
    "        n = len(colors)\n",
    "        value_scaled = value * (n - 1)\n",
    "        idx1 = int(value_scaled)\n",
    "        idx2 = min(idx1 + 1, n - 1)\n",
    "        alpha = value_scaled - idx1\n",
    "        return tuple([int(colors[idx1][i] * (1 - alpha) + colors[idx2][i] * alpha) for i in range(3)])\n",
    "\n",
    "    # 깊이 값을 색상으로 변환\n",
    "    color_map = np.zeros((depth_map.shape[0], depth_map.shape[1], 3), dtype=np.uint8)\n",
    "    for i in range(depth_map.shape[0]):\n",
    "        for j in range(depth_map.shape[1]):\n",
    "            color_map[i, j] = depth_to_color(depth_map_normalized[i, j])\n",
    "    \n",
    "    # RGB와 깊이 이미지를 numpy 배열로 변환\n",
    "    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        o3d.geometry.Image(color_map),\n",
    "        o3d.geometry.Image(depth_map),\n",
    "        convert_rgb_to_intensity=False\n",
    "    )\n",
    "    \n",
    "    # 카메라 매트릭스를 정의합니다 (여기서는 예시로 단순한 매트릭스를 사용)\n",
    "    intrinsic = o3d.camera.PinholeCameraIntrinsic(\n",
    "        o3d.camera.PinholeCameraIntrinsicParameters.PrimeSenseDefault\n",
    "    )\n",
    "    \n",
    "    # 포인트 클라우드를 생성합니다\n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "        rgbd_image,\n",
    "        intrinsic\n",
    "    )\n",
    "    \n",
    "    # 포인트 클라우드를 원점에 맞추기 위해 좌표계를 변환합니다\n",
    "    pcd.transform([[1, 0, 0, 0],\n",
    "                   [0, -1, 0, 0],\n",
    "                   [0, 0, -1, 0],\n",
    "                   [0, 0, 0, 1]])\n",
    "    \n",
    "    return pcd\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # OpenCV 프레임을 PIL 이미지로 변환\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "\n",
    "    # 깊이 맵 추정\n",
    "    depth_map = pipe(pil_image)['depth']\n",
    "    depth_map = np.array(depth_map)\n",
    "\n",
    "    # 포인트 클라우드 생성\n",
    "    pcd = create_point_cloud(rgb_frame, depth_map)\n",
    "    \n",
    "    # 포인트 클라우드 시각화\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    # 결과를 화면에 표시\n",
    "    cv2.imshow('RGB Frame', frame)\n",
    "    \n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 객체 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 깊이 맵을 이용한 객체 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 파일 다운로드 함수\n",
    "def download_file(url, local_filename):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "# 파일 다운로드\n",
    "if not os.path.exists(\"yolov3.weights\"):\n",
    "    download_file(\"https://pjreddie.com/media/files/yolov3.weights\", \"yolov3.weights\")\n",
    "\n",
    "if not os.path.exists(\"yolov3.cfg\"):\n",
    "    download_file(\"https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\", \"yolov3.cfg\")\n",
    "\n",
    "if not os.path.exists(\"coco.names\"):\n",
    "    download_file(\"https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\", \"coco.names\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame slice shape: (219, 276, 3)\n",
      "Depth colormap shape: (219, 282, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Frame slice shape:\", frame[y:y+h, x:x+w].shape)\n",
    "print(\"Depth colormap shape:\", depth_colormap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개선된 객체인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 깊이 맵 유무 객체인식 (상대거리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to Intel/dpt-large and revision e93beec (https://huggingface.co/Intel/dpt-large).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes do not match! Frame slice shape: (235, 360, 3), Depth colormap shape: (236, 360, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes do not match! Frame slice shape: (243, 523, 3), Depth colormap shape: (246, 523, 3)\n",
      "Shapes do not match! Frame slice shape: (251, 509, 3), Depth colormap shape: (252, 509, 3)\n",
      "Shapes do not match! Frame slice shape: (256, 514, 3), Depth colormap shape: (258, 514, 3)\n",
      "Shapes do not match! Frame slice shape: (254, 544, 3), Depth colormap shape: (256, 544, 3)\n",
      "Shapes do not match! Frame slice shape: (254, 576, 3), Depth colormap shape: (258, 576, 3)\n",
      "Shapes do not match! Frame slice shape: (234, 355, 3), Depth colormap shape: (235, 355, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# 깊이 추정 파이프라인 초기화\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\"depth-estimation\", device=device)\n",
    "\n",
    "# 객체 인식 모델 초기화 (YOLOv3 모델 사용)\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# COCO 클래스 이름 불러오기\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# 웹캠을 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # 객체 인식 (깊이 맵 사용하지 않음)\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    frame_no_depth = frame.copy()\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = confidences[i]\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(frame_no_depth, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame_no_depth, label + \" \" + str(round(confidence, 2)), (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "\n",
    "    # 객체 인식 (깊이 맵 사용)\n",
    "    frame_with_depth = frame.copy()\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = confidences[i]\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(frame_with_depth, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame_with_depth, label + \" \" + str(round(confidence, 2)), (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            \n",
    "            # 객체 영역 추출 및 깊이 맵 추정\n",
    "            object_rgb = frame[y:y+h, x:x+w]\n",
    "            object_rgb = cv2.cvtColor(object_rgb, cv2.COLOR_BGR2RGB)\n",
    "            pil_object_image = Image.fromarray(object_rgb)\n",
    "\n",
    "            # 객체 영역의 깊이 맵 추정\n",
    "            depth_result = pipe(pil_object_image)\n",
    "            depth_map = np.array(depth_result['depth'])\n",
    "\n",
    "            # 깊이 맵 시각화\n",
    "            depth_colormap = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "            depth_colormap = cv2.applyColorMap(depth_colormap, cv2.COLORMAP_JET)\n",
    "            depth_colormap = cv2.resize(depth_colormap, (w, h))\n",
    "\n",
    "            # BGR로 변환\n",
    "            depth_colormap = cv2.cvtColor(depth_colormap, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # 크기 및 채널 일치 확인\n",
    "            if frame_with_depth[y:y+h, x:x+w].shape == depth_colormap.shape:\n",
    "                # 깊이 맵을 원본 프레임에 합성\n",
    "                frame_with_depth[y:y+h, x:x+w] = cv2.addWeighted(frame_with_depth[y:y+h, x:x+w], 0.7, depth_colormap, 0.3, 0)\n",
    "\n",
    "                # 객체의 중심 깊이 값 계산\n",
    "                center_depth = depth_map[depth_map.shape[0] // 2, depth_map.shape[1] // 2]\n",
    "                distance = center_depth * 1000  # 깊이 값을 mm로 변환 (예제용으로 1000을 곱함)\n",
    "                cv2.putText(frame_with_depth, f'Distance: {distance:.2f} mm', (x, y - 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            else:\n",
    "                print(f\"Shapes do not match! Frame slice shape: {frame_with_depth[y:y+h, x:x+w].shape}, Depth colormap shape: {depth_colormap.shape}\")\n",
    "\n",
    "    # 좌우 화면 결합\n",
    "    combined_frame = np.hstack((frame_no_depth, frame_with_depth))\n",
    "\n",
    "    # 결과를 화면에 표시\n",
    "    cv2.imshow('Object Detection: No Depth Map (Left) | With Depth Map (Right)', combined_frame)\n",
    "    \n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 손 제스처 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손의 상대거리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to Intel/dpt-large and revision e93beec (https://huggingface.co/Intel/dpt-large).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\lkg\\Desktop\\vscode\\torch\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe Hands 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 깊이 추정 파이프라인 초기화\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "depth_pipe = pipeline(\"depth-estimation\", device=device)\n",
    "\n",
    "# 웹캠 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # BGR 이미지를 RGB로 변환\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # 손 랜드마크 검출\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    depth_frame = frame.copy()  # 깊이 정보 포함한 프레임 복사본\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # 랜드마크 그리기\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            mp_drawing.draw_landmarks(depth_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # 손 랜드마크 좌표 추출 및 제스처 인식\n",
    "            thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "            index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "            middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "            ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "            pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "\n",
    "            thumb_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP]\n",
    "            index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "            middle_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "            ring_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP]\n",
    "            pinky_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP]\n",
    "\n",
    "            thumb_pip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_IP]\n",
    "            index_pip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_PIP]\n",
    "            middle_pip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_PIP]\n",
    "            ring_pip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_PIP]\n",
    "            pinky_pip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_PIP]\n",
    "\n",
    "            # 손 영역 추출\n",
    "            x_min = int(min([lm.x for lm in hand_landmarks.landmark]) * frame.shape[1])\n",
    "            y_min = int(min([lm.y for lm in hand_landmarks.landmark]) * frame.shape[0])\n",
    "            x_max = int(max([lm.x for lm in hand_landmarks.landmark]) * frame.shape[1])\n",
    "            y_max = int(max([lm.y for lm in hand_landmarks.landmark]) * frame.shape[0])\n",
    "\n",
    "            # 좌표가 이미지 범위 내에 있는지 확인\n",
    "            x_min = max(0, x_min)\n",
    "            y_min = max(0, y_min)\n",
    "            x_max = min(frame.shape[1], x_max)\n",
    "            y_max = min(frame.shape[0], y_max)\n",
    "\n",
    "            if x_max - x_min > 0 and y_max - y_min > 0:\n",
    "                hand_rgb = rgb_frame[y_min:y_max, x_min:x_max]\n",
    "                pil_hand_image = Image.fromarray(hand_rgb)\n",
    "\n",
    "                # 손 영역의 깊이 맵 추정\n",
    "                depth_map = depth_pipe(pil_hand_image)['depth']\n",
    "                depth_map = np.array(depth_map)\n",
    "\n",
    "                # 깊이 값을 반전하여 가까운 거리가 더 작게 나타나도록 조정\n",
    "                depth_map = 1 / (depth_map + 1e-6)  # 0으로 나누는 것을 피하기 위해 작은 값 추가\n",
    "\n",
    "                # 깊이 맵 시각화\n",
    "                depth_colormap = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "                depth_colormap = cv2.applyColorMap(depth_colormap, cv2.COLORMAP_JET)\n",
    "                depth_colormap = cv2.resize(depth_colormap, (x_max - x_min, y_max - y_min))\n",
    "                depth_colormap = cv2.cvtColor(depth_colormap, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # 크기 및 채널 일치 확인\n",
    "                if depth_frame[y_min:y_max, x_min:x_max].shape == depth_colormap.shape:\n",
    "                    # 깊이 맵을 원본 프레임에 합성\n",
    "                    depth_frame[y_min:y_max, x_min:x_max] = cv2.addWeighted(\n",
    "                        depth_frame[y_min:y_max, x_min:x_max], 0.7, depth_colormap, 0.3, 0\n",
    "                    )\n",
    "\n",
    "                # 손의 중심 깊이 값 계산\n",
    "                center_depth = depth_map[depth_map.shape[0] // 2, depth_map.shape[1] // 2]\n",
    "                distance = center_depth * 1000  # 깊이 값을 mm로 변환 (예제용으로 1000을 곱함)\n",
    "                cv2.putText(depth_frame, f'Distance: {distance:.2f} mm', (x_min, y_min - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            # 손가락 하트 (small heart) 제스처 인식\n",
    "            thumb_tip_x = thumb_tip.x * frame.shape[1]\n",
    "            thumb_tip_y = thumb_tip.y * frame.shape[0]\n",
    "            index_tip_x = index_tip.x * frame.shape[1]\n",
    "            index_tip_y = index_tip.y * frame.shape[0]\n",
    "\n",
    "            # 엄지와 검지가 서로 가까이 위치\n",
    "            distance_between_tips = np.sqrt((thumb_tip_x - index_tip_x) ** 2 + (thumb_tip_y - index_tip_y) ** 2)\n",
    "\n",
    "            # 엄지와 검지가 교차하는지 확인\n",
    "            if distance_between_tips < 30 and (thumb_tip_x < index_tip_x or thumb_tip_x > index_tip_x):\n",
    "                cv2.putText(frame, 'Small Heart Gesture', (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                cv2.putText(depth_frame, 'Small Heart Gesture', (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            # 뻐큐 손가락 제스처 인식\n",
    "            # 중지가 똑바로 세워져 있고 다른 손가락이 구부러진 상태 확인\n",
    "            if (middle_tip.y < index_tip.y and middle_tip.y < ring_tip.y and \n",
    "                middle_tip.y < pinky_tip.y and middle_tip.y < thumb_tip.y and \n",
    "                middle_tip.y < middle_pip.y and\n",
    "                index_tip.y > index_pip.y and ring_tip.y > ring_pip.y and \n",
    "                pinky_tip.y > pinky_pip.y and thumb_tip.y > thumb_pip.y):\n",
    "                print(\"Detected middle finger gesture. Exiting...\")\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "\n",
    "    # 두 이미지를 나란히 결합\n",
    "    combined_image = np.hstack((frame, depth_frame))\n",
    "    \n",
    "    # 결과를 화면에 표시\n",
    "    cv2.imshow('Hand Gesture Recognition: RGB (Left) vs RGBD (Right)', combined_image)\n",
    "    \n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손 포인트 상대거리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to Intel/dpt-large and revision e93beec (https://huggingface.co/Intel/dpt-large).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\lkg\\Desktop\\vscode\\torch\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe Hands 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 깊이 추정 파이프라인 초기화\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "depth_pipe = pipeline(\"depth-estimation\", device=device)\n",
    "\n",
    "# 웹캠 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # BGR 이미지를 RGB로 변환\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # 손 랜드마크 검출\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    depth_frame = frame.copy()  # 깊이 정보 포함한 프레임 복사본\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # 랜드마크 그리기\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # 손 영역 추출\n",
    "            x_min = int(min([lm.x for lm in hand_landmarks.landmark]) * frame.shape[1])\n",
    "            y_min = int(min([lm.y for lm in hand_landmarks.landmark]) * frame.shape[0])\n",
    "            x_max = int(max([lm.x for lm in hand_landmarks.landmark]) * frame.shape[1])\n",
    "            y_max = int(max([lm.y for lm in hand_landmarks.landmark]) * frame.shape[0])\n",
    "\n",
    "            # 좌표가 이미지 범위 내에 있는지 확인\n",
    "            x_min = max(0, x_min)\n",
    "            y_min = max(0, y_min)\n",
    "            x_max = min(frame.shape[1], x_max)\n",
    "            y_max = min(frame.shape[0], y_max)\n",
    "\n",
    "            if x_max - x_min > 0 and y_max - y_min > 0:\n",
    "                hand_rgb = rgb_frame[y_min:y_max, x_min:x_max]\n",
    "                pil_hand_image = Image.fromarray(hand_rgb)\n",
    "\n",
    "                # 손 영역의 깊이 맵 추정\n",
    "                depth_result = depth_pipe(pil_hand_image)\n",
    "                depth_map = np.array(depth_result['depth'])\n",
    "\n",
    "                # 깊이 값을 반전하지 않음\n",
    "                # 깊이 맵 시각화\n",
    "                depth_colormap = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "                depth_colormap = cv2.applyColorMap(depth_colormap, cv2.COLORMAP_JET)\n",
    "                depth_colormap = cv2.resize(depth_colormap, (x_max - x_min, y_max - y_min))\n",
    "                depth_colormap = cv2.cvtColor(depth_colormap, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # 크기 및 채널 일치 확인\n",
    "                if depth_frame[y_min:y_max, x_min:x_max].shape == depth_colormap.shape:\n",
    "                    # 깊이 맵을 원본 프레임에 합성\n",
    "                    depth_frame[y_min:y_max, x_min:x_max] = cv2.addWeighted(\n",
    "                        depth_frame[y_min:y_max, x_min:x_max], 0.7, depth_colormap, 0.3, 0\n",
    "                    )\n",
    "\n",
    "                # 손의 중심 깊이 값 계산\n",
    "                center_depth = depth_map[depth_map.shape[0] // 2, depth_map.shape[1] // 2]\n",
    "\n",
    "                # 각 손가락 마디에 대한 깊이 정보 점으로 표시\n",
    "                for lm in mp_hands.HandLandmark:\n",
    "                    landmark = hand_landmarks.landmark[lm]\n",
    "                    lm_x = int(landmark.x * frame.shape[1])\n",
    "                    lm_y = int(landmark.y * frame.shape[0])\n",
    "                    if 0 <= lm_x < frame.shape[1] and 0 <= lm_y < frame.shape[0]:\n",
    "                        depth_value = depth_map[int(landmark.y * depth_map.shape[0]), int(landmark.x * depth_map.shape[1])]\n",
    "                        relative_depth = depth_value / center_depth\n",
    "                        cv2.circle(depth_frame, (lm_x, lm_y), 5, (0, 255, 0), -1)\n",
    "                        cv2.putText(depth_frame, f'{relative_depth:.2f}', (lm_x, lm_y - 10),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    # 두 이미지를 나란히 결합\n",
    "    combined_image = np.hstack((frame, depth_frame))\n",
    "    \n",
    "    # 결과를 화면에 표시\n",
    "    cv2.imshow('Hand Depth Information: RGB (Left) vs RGBD (Right)', combined_image)\n",
    "    \n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to Intel/dpt-large and revision e93beec (https://huggingface.co/Intel/dpt-large).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "\n",
    "# 깊이 추정 파이프라인 초기화\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "depth_pipe = pipeline(\"depth-estimation\", device=device)\n",
    "\n",
    "# 객체 인식 모델 초기화 (YOLOv3 모델 사용)\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# COCO 클래스 이름 불러오기\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# MediaPipe Hands 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 웹캠 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # 원본 프레임\n",
    "    original_frame = frame.copy()\n",
    "\n",
    "    # 객체 인식\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    frame_objects = frame.copy()\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = confidences[i]\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(frame_objects, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame_objects, label + \" \" + str(round(confidence, 2)), (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "\n",
    "    # 손 포인트 인식 및 깊이 추정\n",
    "    frame_points = frame.copy()\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame_points, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            x_min = int(min([lm.x for lm in hand_landmarks.landmark]) * frame.shape[1])\n",
    "            y_min = int(min([lm.y for lm in hand_landmarks.landmark]) * frame.shape[0])\n",
    "            x_max = int(max([lm.x for lm in hand_landmarks.landmark]) * frame.shape[1])\n",
    "            y_max = int(max([lm.y for lm in hand_landmarks.landmark]) * frame.shape[0])\n",
    "\n",
    "            x_min = max(0, x_min)\n",
    "            y_min = max(0, y_min)\n",
    "            x_max = min(frame.shape[1], x_max)\n",
    "            y_max = min(frame.shape[0], y_max)\n",
    "\n",
    "            if x_max - x_min > 0 and y_max - y_min > 0:\n",
    "                hand_rgb = rgb_frame[y_min:y_max, x_min:x_max]\n",
    "                pil_hand_image = Image.fromarray(hand_rgb)\n",
    "                depth_result = depth_pipe(pil_hand_image)\n",
    "                depth_map = np.array(depth_result['depth'])\n",
    "\n",
    "                depth_colormap = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "                depth_colormap = cv2.applyColorMap(depth_colormap, cv2.COLORMAP_JET)\n",
    "                depth_colormap = cv2.resize(depth_colormap, (x_max - x_min, y_max - y_min))\n",
    "                depth_colormap = cv2.cvtColor(depth_colormap, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                if frame_points[y_min:y_max, x_min:x_max].shape == depth_colormap.shape:\n",
    "                    frame_points[y_min:y_max, x_min:x_max] = cv2.addWeighted(\n",
    "                        frame_points[y_min:y_max, x_min:x_max], 0.7, depth_colormap, 0.3, 0\n",
    "                    )\n",
    "\n",
    "                center_depth = depth_map[depth_map.shape[0] // 2, depth_map.shape[1] // 2]\n",
    "                for lm in mp_hands.HandLandmark:\n",
    "                    landmark = hand_landmarks.landmark[lm]\n",
    "                    lm_x = int(landmark.x * frame.shape[1])\n",
    "                    lm_y = int(landmark.y * frame.shape[0])\n",
    "                    if 0 <= lm_x < frame.shape[1] and 0 <= lm_y < frame.shape[0]:\n",
    "                        depth_value = depth_map[int(landmark.y * depth_map.shape[0]), int(landmark.x * depth_map.shape[1])]\n",
    "                        relative_depth = depth_value / center_depth\n",
    "                        cv2.circle(frame_points, (lm_x, lm_y), 5, (0, 255, 0), -1)\n",
    "                        cv2.putText(frame_points, f'{relative_depth:.2f}', (lm_x, lm_y - 10),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    # 1행 3열로 이미지 결합\n",
    "    combined_frame = np.hstack((original_frame, frame_objects, frame_points))\n",
    "    \n",
    "    # 결과를 화면에 표시\n",
    "    cv2.imshow('Combined View', combined_frame)\n",
    "    \n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
